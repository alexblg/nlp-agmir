{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_translation.dataset import TagReportDataset, load_pickle\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/home/alex/data/nlp/agmir/transf_processed_data\"\n",
    "#data_path = 'transformer_translation/data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = load_pickle('20200525_splits.pkl')\n",
    "countvec = load_pickle('20200528_countvec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 2000\n",
    "max_seq_length = 96\n",
    "dataset = TagReportDataset(\n",
    "    os.path.join(data_path, 'tags/set_raw.pkl')\n",
    "    ,os.path.join(data_path, 'reports/set.pkl')\n",
    "    ,num_tokens\n",
    "    ,max_seq_length\n",
    "    ,idxs=splits['train']\n",
    "    ,countvec = countvec\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TagReportDataset(\n",
    "    os.path.join(data_path, 'tags/set_raw.pkl')\n",
    "    ,os.path.join(data_path, 'reports/set.pkl')\n",
    "    ,num_tokens\n",
    "    ,max_seq_length\n",
    "    ,idxs=splits['val']\n",
    "    ,countvec = countvec\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_translation.model import LanguageTransformer, ReportTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 + 4#1952#\n",
    "nhead = 8\n",
    "d_model = 587 - (587 % nhead) + nhead\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "pos_dropout = 0.1\n",
    "trans_dropout = 0.1\n",
    "\n",
    "model = ReportTransformer(\n",
    "    vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,\n",
    "    max_seq_length, pos_dropout, trans_dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from transformer_translation.Optim import ScheduledOptim\n",
    "    import torch.nn as nn\n",
    "    from torch.optim import Adam\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_normal_(p)\n",
    "\n",
    "    n_warmup_steps = 4000\n",
    "    optim = ScheduledOptim(\n",
    "        Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        d_model, n_warmup_steps)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsf_infer_utils import prep_transf_inputs, infer\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from tsf_utils import format_list_for_bleu, get_bleu_from_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 20] \t Step [20 / 85] \t Train Loss: 9.181\n",
      "Epoch [1 / 20] \t Step [40 / 85] \t Train Loss: 8.737\n",
      "Epoch [1 / 20] \t Step [60 / 85] \t Train Loss: 8.212\n",
      "Epoch [1 / 20] \t Step [80 / 85] \t Train Loss: 7.880\n",
      "Epoch [1 / 20]:\n",
      "\ttrain BLEU: 0.00%\n",
      "\tval BLEU: 0.00%\n",
      "\n",
      "\n",
      "Epoch [2 / 20] \t Step [20 / 85] \t Train Loss: 7.473\n",
      "Epoch [2 / 20] \t Step [40 / 85] \t Train Loss: 7.190\n",
      "Epoch [2 / 20] \t Step [60 / 85] \t Train Loss: 6.867\n",
      "Epoch [2 / 20] \t Step [80 / 85] \t Train Loss: 6.471\n",
      "Epoch [2 / 20]:\n",
      "\ttrain BLEU: 0.00%\n",
      "\tval BLEU: 0.00%\n",
      "\n",
      "\n",
      "Epoch [3 / 20] \t Step [20 / 85] \t Train Loss: 6.037\n",
      "Epoch [3 / 20] \t Step [40 / 85] \t Train Loss: 5.709\n",
      "Epoch [3 / 20] \t Step [60 / 85] \t Train Loss: 5.346\n",
      "Epoch [3 / 20] \t Step [80 / 85] \t Train Loss: 5.179\n",
      "Epoch [3 / 20]:\n",
      "\ttrain BLEU: 0.00%\n",
      "\tval BLEU: 0.00%\n",
      "\n",
      "\n",
      "Epoch [4 / 20] \t Step [20 / 85] \t Train Loss: 4.754\n",
      "Epoch [4 / 20] \t Step [40 / 85] \t Train Loss: 4.394\n",
      "Epoch [4 / 20] \t Step [60 / 85] \t Train Loss: 4.397\n",
      "Epoch [4 / 20] \t Step [80 / 85] \t Train Loss: 4.241\n",
      "Epoch [4 / 20]:\n",
      "\ttrain BLEU: 7.13%\n",
      "\tval BLEU: 5.58%\n",
      "\n",
      "\n",
      "Epoch [5 / 20] \t Step [20 / 85] \t Train Loss: 3.861\n",
      "Epoch [5 / 20] \t Step [40 / 85] \t Train Loss: 3.640\n",
      "Epoch [5 / 20] \t Step [60 / 85] \t Train Loss: 3.548\n",
      "Epoch [5 / 20] \t Step [80 / 85] \t Train Loss: 3.384\n",
      "Epoch [5 / 20]:\n",
      "\ttrain BLEU: 12.03%\n",
      "\tval BLEU: 9.55%\n",
      "\n",
      "\n",
      "Epoch [6 / 20] \t Step [20 / 85] \t Train Loss: 3.239\n",
      "Epoch [6 / 20] \t Step [40 / 85] \t Train Loss: 3.120\n",
      "Epoch [6 / 20] \t Step [60 / 85] \t Train Loss: 3.019\n",
      "Epoch [6 / 20] \t Step [80 / 85] \t Train Loss: 2.917\n",
      "Epoch [6 / 20]:\n",
      "\ttrain BLEU: 16.48%\n",
      "\tval BLEU: 12.56%\n",
      "\n",
      "\n",
      "Epoch [7 / 20] \t Step [20 / 85] \t Train Loss: 2.985\n",
      "Epoch [7 / 20] \t Step [40 / 85] \t Train Loss: 2.717\n",
      "Epoch [7 / 20] \t Step [60 / 85] \t Train Loss: 2.647\n",
      "Epoch [7 / 20] \t Step [80 / 85] \t Train Loss: 2.632\n",
      "Epoch [7 / 20]:\n",
      "\ttrain BLEU: 20.67%\n",
      "\tval BLEU: 16.20%\n",
      "\n",
      "\n",
      "Epoch [8 / 20] \t Step [20 / 85] \t Train Loss: 2.477\n",
      "Epoch [8 / 20] \t Step [40 / 85] \t Train Loss: 2.602\n",
      "Epoch [8 / 20] \t Step [60 / 85] \t Train Loss: 2.445\n",
      "Epoch [8 / 20] \t Step [80 / 85] \t Train Loss: 2.582\n",
      "Epoch [8 / 20]:\n",
      "\ttrain BLEU: 22.69%\n",
      "\tval BLEU: 17.32%\n",
      "\n",
      "\n",
      "Epoch [9 / 20] \t Step [20 / 85] \t Train Loss: 2.332\n",
      "Epoch [9 / 20] \t Step [40 / 85] \t Train Loss: 2.386\n",
      "Epoch [9 / 20] \t Step [60 / 85] \t Train Loss: 2.230\n",
      "Epoch [9 / 20] \t Step [80 / 85] \t Train Loss: 2.259\n",
      "Epoch [9 / 20]:\n",
      "\ttrain BLEU: 26.70%\n",
      "\tval BLEU: 20.78%\n",
      "\n",
      "\n",
      "Epoch [10 / 20] \t Step [20 / 85] \t Train Loss: 2.145\n",
      "Epoch [10 / 20] \t Step [40 / 85] \t Train Loss: 2.121\n",
      "Epoch [10 / 20] \t Step [60 / 85] \t Train Loss: 2.169\n",
      "Epoch [10 / 20] \t Step [80 / 85] \t Train Loss: 2.051\n",
      "Epoch [10 / 20]:\n",
      "\ttrain BLEU: 29.25%\n",
      "\tval BLEU: 22.19%\n",
      "\n",
      "\n",
      "Epoch [11 / 20] \t Step [20 / 85] \t Train Loss: 2.099\n",
      "Epoch [11 / 20] \t Step [40 / 85] \t Train Loss: 2.031\n",
      "Epoch [11 / 20] \t Step [60 / 85] \t Train Loss: 1.933\n",
      "Epoch [11 / 20] \t Step [80 / 85] \t Train Loss: 1.948\n",
      "Epoch [11 / 20]:\n",
      "\ttrain BLEU: 32.95%\n",
      "\tval BLEU: 25.74%\n",
      "\n",
      "\n",
      "Epoch [12 / 20] \t Step [20 / 85] \t Train Loss: 1.892\n",
      "Epoch [12 / 20] \t Step [40 / 85] \t Train Loss: 1.793\n",
      "Epoch [12 / 20] \t Step [60 / 85] \t Train Loss: 1.994\n",
      "Epoch [12 / 20] \t Step [80 / 85] \t Train Loss: 1.827\n",
      "Epoch [12 / 20]:\n",
      "\ttrain BLEU: 35.06%\n",
      "\tval BLEU: 27.07%\n",
      "\n",
      "\n",
      "Epoch [13 / 20] \t Step [20 / 85] \t Train Loss: 1.858\n",
      "Epoch [13 / 20] \t Step [40 / 85] \t Train Loss: 1.780\n",
      "Epoch [13 / 20] \t Step [60 / 85] \t Train Loss: 1.773\n",
      "Epoch [13 / 20] \t Step [80 / 85] \t Train Loss: 1.823\n",
      "Epoch [13 / 20]:\n",
      "\ttrain BLEU: 35.28%\n",
      "\tval BLEU: 26.04%\n",
      "\n",
      "\n",
      "Epoch [14 / 20] \t Step [20 / 85] \t Train Loss: 1.852\n",
      "Epoch [14 / 20] \t Step [40 / 85] \t Train Loss: 1.838\n",
      "Epoch [14 / 20] \t Step [60 / 85] \t Train Loss: 1.584\n",
      "Epoch [14 / 20] \t Step [80 / 85] \t Train Loss: 1.476\n",
      "Epoch [14 / 20]:\n",
      "\ttrain BLEU: 38.37%\n",
      "\tval BLEU: 29.19%\n",
      "\n",
      "\n",
      "Epoch [15 / 20] \t Step [20 / 85] \t Train Loss: 1.686\n",
      "Epoch [15 / 20] \t Step [40 / 85] \t Train Loss: 1.441\n",
      "Epoch [15 / 20] \t Step [60 / 85] \t Train Loss: 1.780\n",
      "Epoch [15 / 20] \t Step [80 / 85] \t Train Loss: 1.603\n",
      "Epoch [15 / 20]:\n",
      "\ttrain BLEU: 42.23%\n",
      "\tval BLEU: 31.68%\n",
      "\n",
      "\n",
      "Epoch [16 / 20] \t Step [20 / 85] \t Train Loss: 1.615\n",
      "Epoch [16 / 20] \t Step [40 / 85] \t Train Loss: 1.406\n",
      "Epoch [16 / 20] \t Step [60 / 85] \t Train Loss: 1.572\n",
      "Epoch [16 / 20] \t Step [80 / 85] \t Train Loss: 1.587\n",
      "Epoch [16 / 20]:\n",
      "\ttrain BLEU: 43.23%\n",
      "\tval BLEU: 32.60%\n",
      "\n",
      "\n",
      "Epoch [17 / 20] \t Step [20 / 85] \t Train Loss: 1.326\n",
      "Epoch [17 / 20] \t Step [40 / 85] \t Train Loss: 1.560\n",
      "Epoch [17 / 20] \t Step [60 / 85] \t Train Loss: 1.372\n",
      "Epoch [17 / 20] \t Step [80 / 85] \t Train Loss: 1.614\n",
      "Epoch [17 / 20]:\n",
      "\ttrain BLEU: 44.91%\n",
      "\tval BLEU: 33.57%\n",
      "\n",
      "\n",
      "Epoch [18 / 20] \t Step [20 / 85] \t Train Loss: 1.441\n",
      "Epoch [18 / 20] \t Step [40 / 85] \t Train Loss: 1.217\n",
      "Epoch [18 / 20] \t Step [60 / 85] \t Train Loss: 1.588\n",
      "Epoch [18 / 20] \t Step [80 / 85] \t Train Loss: 1.358\n",
      "Epoch [18 / 20]:\n",
      "\ttrain BLEU: 47.12%\n",
      "\tval BLEU: 34.98%\n",
      "\n",
      "\n",
      "Epoch [19 / 20] \t Step [20 / 85] \t Train Loss: 1.219\n",
      "Epoch [19 / 20] \t Step [40 / 85] \t Train Loss: 1.343\n",
      "Epoch [19 / 20] \t Step [60 / 85] \t Train Loss: 1.314\n",
      "Epoch [19 / 20] \t Step [80 / 85] \t Train Loss: 1.418\n",
      "Epoch [19 / 20]:\n",
      "\ttrain BLEU: 47.95%\n",
      "\tval BLEU: 34.32%\n",
      "\n",
      "\n",
      "Epoch [20 / 20] \t Step [20 / 85] \t Train Loss: 1.390\n",
      "Epoch [20 / 20] \t Step [40 / 85] \t Train Loss: 1.280\n",
      "Epoch [20 / 20] \t Step [60 / 85] \t Train Loss: 1.199\n",
      "Epoch [20 / 20] \t Step [80 / 85] \t Train Loss: 1.237\n",
      "Epoch [20 / 20]:\n",
      "\ttrain BLEU: 49.57%\n",
      "\tval BLEU: 35.74%\n",
      "\n",
      "\n",
      "CPU times: user 3h 24min 38s, sys: 7min 14s, total: 3h 31min 53s\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print_every = 20\n",
    "    num_epochs = 20\n",
    "    early_stopping_flag = True\n",
    "\n",
    "    lowest_val = 1e9\n",
    "    val_losses = []\n",
    "    total_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, (src, src_key_padding_mask, tgt, tgt_key_padding_mask) in enumerate(iter(loader)):\n",
    "            total_step += 1\n",
    "\n",
    "            src, src_key_padding_mask, tgt, tgt_key_padding_mask, memory_key_padding_mask, tgt_inp, tgt_out, tgt_mask = prep_transf_inputs(\n",
    "                src, src_key_padding_mask, tgt, tgt_key_padding_mask, device)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
    "            loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step_and_update_lr()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if step % print_every == print_every - 1:\n",
    "                print(f'Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(loader)}] \\t '\n",
    "                      'Train Loss: {:.3f}'.format(total_loss / print_every))\n",
    "                total_loss = 0\n",
    "                \n",
    "        if early_stopping_flag:\n",
    "            model.eval()\n",
    "            print(f'Epoch [{epoch + 1} / {num_epochs}]:')\n",
    "            print('{} BLEU: {:.2%}'.format(\n",
    "                '\\ttrain', get_bleu_from_loader(model, loader)))\n",
    "            print('{} BLEU: {:.2%}'.format(\n",
    "                '\\tval', get_bleu_from_loader(model, val_loader)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the model with our previous way to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsf_utils import get_tk_from_proba, format_list_for_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 1.02 s, total: 17.8 s\n",
      "Wall time: 692 ms\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    pred_list = []\n",
    "    tgt_list = []\n",
    "    device = model.embed_tgt.weight.device\n",
    "    \n",
    "    for (src, src_key_padding_mask, tgt, tgt_key_padding_mask) in iter(val_loader):\n",
    "        \n",
    "        # prepare inputs\n",
    "        src, src_key_padding_mask, tgt, tgt_key_padding_mask, memory_key_padding_mask, tgt_inp, tgt_out, tgt_mask = prep_transf_inputs(\n",
    "            src, src_key_padding_mask, tgt, tgt_key_padding_mask, device)\n",
    "\n",
    "        # run inference\n",
    "        outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
    "\n",
    "        # get predictions from proba\n",
    "        pred = get_tk_from_proba(outputs)\n",
    "        \n",
    "        # get pred and ground truth ready for metric eval\n",
    "        pred_list += [list(pred[row, :].cpu().numpy()) for row in range(pred.shape[0])]\n",
    "        tgt_list += [list(tgt[row, :].cpu().numpy()) for row in range(pred.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tval BLEU: 35.74%\n"
     ]
    }
   ],
   "source": [
    "pred_list_bleu, tgt_list_bleu = format_list_for_bleu(pred_list, tgt_list)\n",
    "print('{} BLEU: {:.2%}'.format(\n",
    "                '\\tval',bleu_score(pred_list_bleu, tgt_list_bleu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the new way to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_translation.translate_sentence import gen_nopeek_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(model, src, pred_sentence):\n",
    "    tgt = torch.tensor(pred_sentence).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # prepare inputs\n",
    "    src = src.to(device)\n",
    "    tgt = tgt[0].to(device)\n",
    "    tgt_mask = gen_nopeek_mask(tgt.shape[1]).to(device)\n",
    "\n",
    "    # run inference\n",
    "    return model(src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_translation.dataset import IDX_EOS, IDX_SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS:  ['normal']\n",
      "TARGET:  SOS no active cardiopulmonary disease . EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "PREDICTION:  SOS no acute cardiopulmonary abnormality . . the lungs are clear bilaterally . specifically no evidence of focal consolidation pneumothorax or pleural effusion . . cardio mediastinal silhouette is unremarkable . visualized osseous structures of the thorax are without acute abnormality . EOS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data example\n",
    "(src, src_key_padding_mask, tgt, tgt_key_padding_mask) = next(iter(val_loader))\n",
    "\n",
    "# select single sentence\n",
    "src = src[:,[0],:]\n",
    "tgt = tgt[:,[0],:]\n",
    "\n",
    "# replace by initial tgt for out-of-sample decoding\n",
    "pred_sentence = [IDX_SOS]\n",
    "\n",
    "# run out-of-sample decoding\n",
    "i = 0\n",
    "while int(pred_sentence[-1]) != IDX_EOS and i < max_seq_length:\n",
    "    output_tk = get_tk_from_proba(\n",
    "        forward_model(model, src, pred_sentence))\n",
    "    pred_sentence.append(output_tk[0][-1].item())\n",
    "    output_tk = torch.tensor(pred_sentence).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    i += 1\n",
    "\n",
    "print('TAGS: ',[tags_index2word[idx.item()] for idx in src[0,0,:].nonzero()])\n",
    "print_nl_pred_vs_tgt(\n",
    "    [list(output_tk[0][row, :].cpu().numpy()) for row in range(output_tk[0].shape[0])]\n",
    "    ,[list(tgt[0][row, :].cpu().numpy()) for row in range(tgt[0].shape[0])]\n",
    "    ,reports_index2word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assess perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TagReportDataset(\n",
    "    os.path.join(data_path, 'tags/set_raw.pkl')\n",
    "    ,os.path.join(data_path, 'reports/set.pkl')\n",
    "    ,num_tokens\n",
    "    ,max_seq_length\n",
    "    ,idxs=splits['test']\n",
    "    ,countvec = countvec\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 1.32 s, total: 20.8 s\n",
      "Wall time: 906 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_list, tgt_list = infer(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test BLEU: 40.81%\n",
      "CPU times: user 1.58 s, sys: 20 ms, total: 1.6 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_list_bleu, tgt_list_bleu = format_list_for_bleu(pred_list, tgt_list)\n",
    "print('{} BLEU: {:.2%}'.format(\n",
    "                'test', bleu_score(pred_list_bleu, tgt_list_bleu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create vocab dicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_index2word = dict(zip(range(len(dataset.countvec.vocabulary)), dataset.countvec.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_translation.dataset import load_report_voc\n",
    "reports_index2word = load_report_voc(\n",
    "    os.path.join(data_path, 'reports', 'voc.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assess nat lg perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsf_utils import print_nl_pred_vs_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "k = 10\n",
    "sel_idx = random.sample(range(len(pred_list)), k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET:   normal chest no evidence of tuberculosis heart size normal . lungs are clear . xxxx are normal . no pneumonia effusions edema pneumothorax adenopathy nodules or masses . .\n",
      "PREDICTION:  no chest heart evidence of tuberculosis heart size normal . lungs are clear . xxxx are normal . no pneumonia effusions edema pneumothorax adenopathy nodules or masses . .\n",
      "\n",
      "\n",
      "TARGET:   no acute cardiopulmonary abnormality . mediastinal contours are normal . heart size is within normal limits . multiple scattered calcified pulmonary nodules xxxx sequela of prior granulomatous disease . otherwise lungs are clear . . there is no pneumothorax or large pleural effusion . no bony abnormality . .\n",
      "PREDICTION:   acute cardiopulmonary abnormality . . contours are within . no size is within normal limits . no calcified calcified granulomas nodules are . of prior granulomatous disease . no lungs are clear without no no is no pneumothorax or large pleural effusion . there acute abnormality . .\n",
      "\n",
      "\n",
      "TARGET:   no acute findings . cardiac and mediastinal contours are within normal limits . the lungs are clear . bony structures are intact . .\n",
      "PREDICTION:  no acute cardiopulmonary . cardiac and mediastinal contours are within normal limits . the lungs are clear . bony structures are intact . .\n",
      "\n",
      "\n",
      "TARGET:   exam quality limited by hypoinflation and rotation . considering technical factors heart size xxxx mildly enlarged aortic calcifications and ectasia tortuosity mediastinal calcifications suggest a previous granulomatous process . no focal alveolar consolidation no definite pleural effusion seen . bronchovascular crowding without typical findings of pulmonary edema . .\n",
      "PREDICTION:   quality limited by body and no . heart technical factors xxxx size and within enlarged . ectasia and aortic tortuosity stable contours suggest a previous granulomatous process . no focal alveolar consolidation no definite pleural effusion seen . no crowding without typical findings of pulmonary edema . .\n",
      "\n",
      "\n",
      "TARGET:   no acute cardiopulmonary disease . . the cardiomediastinal silhouette is normal size and configuration . pulmonary vasculature within normal limits . the lungs are well aerated . there is no pneumothorax pleural effusion or focal consolidation . .\n",
      "PREDICTION:  no acute cardiopulmonary abnormality . the the cardiomediastinal silhouette is normal in and configuration . pulmonary vasculature within normal limits . the lungs are well aerated . there is no pneumothorax pleural effusion or focal consolidation . bony\n",
      "\n",
      "\n",
      "TARGET:   copd . no acute pulmonary disease . there is hyperinflation of the lungs appear to be clear . there is no pleural effusion or the heart is normal . there are atherosclerotic changes of the aorta . the skeletal structures are normal . .\n",
      "PREDICTION:   . no acute pulmonary disease . the is a . the lungs . to be clear . there is no pleural effusion or pneumothorax heart and not in arthritic are no changes of the aorta . arthritic skeletal structures are normal . .\n",
      "\n",
      "\n",
      "TARGET:   low lung volumes otherwise clear . the cardiomediastinal silhouette is normal in size and contour . no focal consolidation pneumothorax or large pleural effusion . normal xxxx . xxxx cholecystectomy . .\n",
      "PREDICTION:  no lung volumes with no . the cardiomediastinal silhouette is normal in size and contour . no focal consolidation pneumothorax or large pleural effusion . calcified xxxx . . foreign . .\n",
      "\n",
      "\n",
      "TARGET:   minimal xxxx left base atelectasis infiltrate . otherwise stable exam . mild cardiomegaly unchanged . stable superior mediastinal contour with tortuous aorta . normal pulmonary vascularity . unchanged elevated right hemidiaphragm with minimal right base subsegmental atelectasis . minimal xxxx left basal airspace opacity . unchanged blunting of the right lateral costophrenic xxxx scarring versus xxxx effusion . no pneumothorax . no acute osseous findings . .\n",
      "PREDICTION:   left right basilar airspace . . the unremarkable appearance . stable cardiomegaly . . the cardiomegaly mediastinal contour . unchanged aorta . no pulmonary vascularity . no right right hemidiaphragm . xxxx right pleural atelectasis atelectasis or no right right pleural airspace disease . no blunting of the right costophrenic costophrenic xxxx . . atelectasis . . no pneumothorax or no acute bony findings . .\n",
      "\n",
      "\n",
      "TARGET:   mild cardiomegaly no acute pulmonary findings mild cardiomegaly stable mediastinal contours . no focal alveolar consolidation no definite pleural effusion seen . mild bronchovascular crowding without typical findings of pulmonary edema . .\n",
      "PREDICTION:  cardiomegaly cardiomegaly . acute pulmonary findings heart cardiomegaly . mediastinal contours . stable focal alveolar consolidation no definite pleural effusion seen . no right crowding without typical findings of pulmonary edema . .\n",
      "\n",
      "\n",
      "TARGET:   mild blunted right costophrenic xxxx which could be due to xxxx effusion or scarring . the heart and mediastinum are normal . the lungs are clear . there is mild blunting of the right costophrenic xxxx . there is no infiltrate mass or pneumothorax . the right internal jugular catheter has been removed . .\n",
      "PREDICTION:   bibasilar left costophrenic xxxx . may represent secondary to atelectasis effusion or atelectasis . no cardiomediastinal size mediastinum are within . the lungs are clear . there is no xxxx of the costophrenic costophrenic xxxx . there is no pneumothorax or or pneumothorax . . bony costophrenic jugular catheter has been removed . .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_nl_pred_vs_tgt(\n",
    "                [pred_list[i] for i in sel_idx]\n",
    "                ,[tgt_list[i] for i in sel_idx]\n",
    "                ,reports_index2word\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcic",
   "language": "python",
   "name": "bcic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
